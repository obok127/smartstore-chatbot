from __future__ import annotations
from fastapi import FastAPI, HTTPException
from fastapi.responses import StreamingResponse, FileResponse, HTMLResponse
from fastapi.middleware.cors import CORSMiddleware
from typing import List, Dict, Any
import json, uuid, os
import re
import pandas as pd
from urllib.parse import quote_plus
from .schemas import ChatRequest, ChatChunk, ChatResponse, IndexRequest, ConversationHistory, Message
from .retriever import Retriever
from .memory import ConversationMemory
from .llm import LLM, build_prompt
from .config import settings
from .guard import is_on_topic, detect_intent
from .prompts import build_fallback_response

# ë°°í¬ í™˜ê²½ì—ì„œ ë°ì´í„° ë‹¤ìš´ë¡œë“œ
def ensure_data_exists():
    """ë°°í¬ í™˜ê²½ì—ì„œ í•„ìš”í•œ ë°ì´í„° íŒŒì¼ì´ ì¡´ì¬í•˜ëŠ”ì§€ í™•ì¸í•˜ê³  ë‹¤ìš´ë¡œë“œí•©ë‹ˆë‹¤."""
    data_path = "data/final_result.pkl"
    
    if not os.path.exists(data_path):
        print("ğŸ“¥ ë°°í¬ í™˜ê²½ì—ì„œ ë°ì´í„° íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤. ë‹¤ìš´ë¡œë“œë¥¼ ì‹œë„í•©ë‹ˆë‹¤...")
        try:
            # ë°ì´í„° ë‹¤ìš´ë¡œë“œ ìŠ¤í¬ë¦½íŠ¸ ì‹¤í–‰
            import subprocess
            result = subprocess.run(["python", "scripts/download_data.py"], 
                                  capture_output=True, text=True)
            if result.returncode == 0:
                print("âœ… ë°ì´í„° ë‹¤ìš´ë¡œë“œ ì™„ë£Œ")
            else:
                print(f"âŒ ë°ì´í„° ë‹¤ìš´ë¡œë“œ ì‹¤íŒ¨: {result.stderr}")
        except Exception as e:
            print(f"âŒ ë°ì´í„° ë‹¤ìš´ë¡œë“œ ìŠ¤í¬ë¦½íŠ¸ ì‹¤í–‰ ì‹¤íŒ¨: {e}")
            print("âš ï¸  ìˆ˜ë™ìœ¼ë¡œ data/final_result.pkl íŒŒì¼ì„ ì¶”ê°€í•´ì£¼ì„¸ìš”.")

app = FastAPI(title="SmartStore FAQ RAG (no-langchain)",
              description="Streaming RAG chatbot for Naver SmartStore FAQ",
              version="1.3.0")

# Enable CORS for local development and file/other origins
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=False,
    allow_methods=["*"],
    allow_headers=["*"],
)

retriever = Retriever()
memory = ConversationMemory()
llm = LLM()


def make_help_links(q: str):
    base = "https://help.sell.smartstore.naver.com/index.help"
    search = f"https://help.sell.smartstore.naver.com/faq/search.help?categoryNo=0&searchKeyword={quote_plus(q)}"
    return base, search


@app.get("/health")
def health():
    return {"status": "ok"}

@app.get("/")
def serve_frontend_root():
    import os
    root = os.path.dirname(os.path.dirname(__file__))
    html_path = os.path.join(root, "smart_store_faq_chat_frontend_index.html")
    if os.path.exists(html_path):
        return FileResponse(html_path, media_type="text/html; charset=utf-8")
    return HTMLResponse("<h1>Frontend file not found</h1>", status_code=404)


@app.post("/index")
def index(req: IndexRequest):
    path = req.pkl_path
    if not os.path.exists(path):
        raise HTTPException(status_code=400, detail=f"File not found: {path}")

    if req.reset:
        retriever.reset()

    # Load .pkl (supports DataFrame, list[dict], list[(q,a)], dict[q->str|dict])
    try:
        obj = pd.read_pickle(path)
        if isinstance(obj, pd.DataFrame):
            df = obj
        elif isinstance(obj, list):
            if len(obj) > 0 and isinstance(obj[0], dict):
                df = pd.DataFrame(obj)
            elif len(obj) > 0 and isinstance(obj[0], (list, tuple)) and len(obj[0]) >= 2:
                # Assume (question, answer, ...)
                df = pd.DataFrame(obj, columns=["question", "answer"])  # extra cols ignored
            else:
                raise ValueError("Unsupported list format in pkl. Expect list of dicts or list of (q,a)")
        elif isinstance(obj, dict):
            rows = []
            for k, v in obj.items():
                if isinstance(v, str):
                    rows.append({"question": str(k), "answer": v})
                elif isinstance(v, dict):
                    q = v.get("question") or v.get("ì§ˆë¬¸") or str(k)
                    a = v.get("answer") or v.get("ë‹µë³€") or v.get("ë‚´ìš©") or ""
                    url = v.get("url") or v.get("ë§í¬") or ""
                    title = v.get("title") or v.get("ì œëª©") or str(k)
                    cat = v.get("category") or v.get("ì¹´í…Œê³ ë¦¬") or ""
                    rows.append({
                        "question": str(q),
                        "answer": str(a),
                        "url": str(url),
                        "title": str(title),
                        "category": str(cat),
                    })
                else:
                    rows.append({"question": str(k), "answer": str(v)})
            df = pd.DataFrame(rows)
        else:
            raise ValueError(f"Unsupported object type: {type(obj)}")
    except Exception as e:
        raise HTTPException(status_code=400, detail=f"Failed to load pkl: {e}")

    # Try to normalize column names
    rename_map = {
        "ì§ˆë¬¸": "question",
        "ë¬¸ì˜": "question",
        "Q": "question",
        "ë‹µë³€": "answer",
        "A": "answer",
        "ë§í¬": "url",
        "ì œëª©": "title",
        "ì¹´í…Œê³ ë¦¬": "category",
    }
    for k, v in rename_map.items():
        if k in df.columns and v not in df.columns:
            df[v] = df[k]

    for col in ["question", "answer", "url", "title", "category"]:
        if col not in df.columns:
            df[col] = ""

    docs = []
    for i, row in df.iterrows():
        _id = str(row.get("id") or row.get("url") or f"doc-{i}")
        q = str(row.get("question","")).strip()
        a = str(row.get("answer","")).strip()
        title = str(row.get("title","")).strip() or q[:40]
        url = str(row.get("url","")).strip()
        cat = str(row.get("category","")).strip()
        # í•˜ë‚˜ì˜ FAQ(ì§ˆë¬¸+ë‹µë³€)ë¥¼ í•œ ì²­í¬ë¡œ ì €ì¥
        text = (q + "\n" + a).strip()
        if not text:
            continue
        docs.append({"id": _id, "text": text, "title": title, "url": url, "category": cat})

    if not docs:
        raise HTTPException(status_code=400, detail="No valid docs in the provided pkl.")

    result = retriever.upsert(docs)
    # í†µê³„ ì •ë³´ ì¶”ê°€í•˜ì—¬ ë°˜í™˜
    try:
        chroma_count = retriever.collection.count()
    except Exception:
        chroma_count = 0
    bm25_loaded = bool(retriever._bm25)
    doc_map_size = len(getattr(retriever, "_doc_map", {}) or {})
    samples = []
    try:
        # ì œëª© ìƒ˜í”Œ ìµœëŒ€ 5ê°œ
        for i, d in enumerate(list((getattr(retriever, "_doc_map", {}) or {}).values())[:5]):
            samples.append(d.get("title", ""))
    except Exception:
        samples = []

    out = {
        **result,
        "chroma_count": chroma_count,
        "bm25_loaded": bm25_loaded,
        "doc_map_size": doc_map_size,
        "samples": samples,
    }
    return out

@app.post("/chat/stream")
def chat_stream(req: ChatRequest):
    conv_id = req.conversation_id or str(uuid.uuid4())
    user_msg = req.message.strip()
    top_k = req.top_k or settings.top_k

    if not user_msg:
        raise HTTPException(status_code=400, detail="Empty message.")

    memory.add(conv_id, "user", user_msg)
    history_text = memory.format_as_chat(conv_id, limit=12)
    ctx = retriever.retrieve(user_msg, k=top_k)

    # -------- Intent routing --------
    intent = detect_intent(user_msg, ctx, settings.score_threshold)

    def simple_stream(text: str):
        def gen():
            chunk_size = 90
            for i in range(0, len(text), chunk_size):
                yield f"data: {json.dumps({'type':'token','content': text[i:i+chunk_size]})}\n\n"
            yield "event: done\n"
            yield "data: {}\n\n"
        return StreamingResponse(gen(), media_type="text/event-stream")

    if intent == "greeting":
        msg = (
            "ì•ˆë…•í•˜ì„¸ìš”! ìŠ¤ë§ˆíŠ¸ìŠ¤í† ì–´ FAQ ì „ìš© ì±—ë´‡ì…ë‹ˆë‹¤.\n"
            "ë°”ë¡œ ì´ë ‡ê²Œ ë¬¼ì–´ë³´ì„¸ìš”:\n"
            "- ë¯¸ì„±ë…„ìë„ ìŠ¤ë§ˆíŠ¸ìŠ¤í† ì–´ë¥¼ ê°œì„¤í•  ìˆ˜ ìˆë‚˜ìš”?\n"
            "- ìƒí’ˆ ë“±ë¡ì€ ì–´ë–¤ ìˆœì„œë¡œ í•˜ë‚˜ìš”?\n"
            "- ì •ì‚° ì£¼ê¸°/ìˆ˜ìˆ˜ë£ŒëŠ” ì–´ë–»ê²Œ ë˜ë‚˜ìš”?\n"
        )
        return simple_stream(msg)

    if intent == "thanks":
        msg = "ë„ì›€ì´ ë˜ì—ˆë‹¤ë‹ˆ ë‹¤í–‰ì´ì—ìš”! ìŠ¤ë§ˆíŠ¸ìŠ¤í† ì–´ì— ëŒ€í•´ ë” ê¶ê¸ˆí•œ ì ì´ ìˆìœ¼ë©´ í¸í•˜ê²Œ ë¬¼ì–´ë³´ì„¸ìš”."
        return simple_stream(msg)

    if intent == "help":
        msg = (
            "ì‚¬ìš©ë²• ì•ˆë‚´ ë“œë¦´ê²Œìš”. ì´ ì±—ë´‡ì€ **ìŠ¤ë§ˆíŠ¸ìŠ¤í† ì–´ ê³µì‹ FAQ**ë§Œì„ ê·¼ê±°ë¡œ ì§§ê²Œ ë‹µí•©ë‹ˆë‹¤.\n"
            "ì˜ˆì‹œ:\n"
            "- \"ë¯¸ì„±ë…„ìë„ ê°œì„¤ ê°€ëŠ¥?\"\n"
            "- \"êµ­ë‚´ í†µì‹ íŒë§¤ì—… ì‹ ê³ ê°€ í•„ìˆ˜ì¸ê°€ìš”?\"\n"
            "- \"ìƒí’ˆ ì¼ê´„ë“±ë¡ ë°©ë²• ì•Œë ¤ì¤˜\"\n"
            "êµ¬ì²´ì ìœ¼ë¡œ ë¬¼ì–´ë³¼ìˆ˜ë¡ ë” ì •í™•íˆ ì•ˆë‚´ë“œë ¤ìš”."
        )
        return simple_stream(msg)

    if intent == "offtopic":
        msg = (
            "ì €ëŠ” ìŠ¤ë§ˆíŠ¸ìŠ¤í† ì–´ FAQë¥¼ ìœ„í•œ ì±—ë´‡ì…ë‹ˆë‹¤. ìŠ¤ë§ˆíŠ¸ìŠ¤í† ì–´ ê´€ë ¨ ì§ˆë¬¸ì„ ë¶€íƒë“œë ¤ìš”.\n\n"
            "ì˜ˆì‹œ: ìƒí’ˆ ë“±ë¡ ì ˆì°¨, ìˆ˜ìˆ˜ë£Œ/ì •ì‚°, ì •ì±…/ì¸ì¦, ë°°ì†¡/êµí™˜/í™˜ë¶ˆ ë“±"
        )
        return simple_stream(msg)

    # -------- SMART intent: go through LLM (with fallback) --------
    messages = build_prompt(ctx, history_text, user_msg)

    def sse_gen():
        buffer = []
        try:
            for delta in llm.stream_answer(messages):
                buffer.append(delta)
                yield f"data: {json.dumps({'type':'token','content': delta})}\n\n"
            final = "".join(buffer)
            
            # í›„ì²˜ë¦¬ í›… ì ìš©
            final = _strip_persona(final)
            final = _dedup_followups(final)
            # citations ì£¼ì…/ì¹˜í™˜
            cits = _build_citations(ctx)
            final = re.sub(r"<citations>.*?</citations>", cits, final, flags=re.S) if "<citations>" in final else (final + "\n\n" + cits)
            
            if final.strip():
                memory.add(conv_id, "assistant", final[:1500])
            yield "event: done\n"
            yield "data: {}\n\n"
        except Exception as e:
            # LLM ì—ëŸ¬ ë¡œê¹… ì¶”ê°€
            import logging
            logging.exception("LLM error", exc_info=True)
            # Fallback: ì»¨í…ìŠ¤íŠ¸ ê¸°ë°˜ ê°„ë‹¨ ë‹µë³€ (SMART ì˜ë„ì—ì„œë§Œ ì‚¬ìš©)
            try:
                parts = []
                if ctx:
                    best = ctx[0]
                    full = (best.get('text', '') or '').replace('passage: ', '')
                    if '\n' in full:
                        _, ans = full.split('\n', 1)
                    else:
                        ans = full
                    ans = ans.strip()
                    # Citations
                    cits = []
                    for d in ctx[:3]:
                        t = d.get('title', '') or 'FAQ'
                        u = d.get('url', '') or ''
                        if u:
                            cits.append((t, u))
                    
                    # ìƒˆë¡œìš´ í´ë°± ì‘ë‹µ ë¹Œë” ì‚¬ìš©
                    text = build_fallback_response(ans, cits)
                else:
                    text = "í˜„ì¬ ì°¸ê³ í•  ì»¨í…ìŠ¤íŠ¸ê°€ ì—†ìŠµë‹ˆë‹¤. ë‚˜ì¤‘ì— ë‹¤ì‹œ ì‹œë„í•´ì£¼ì„¸ìš”."
                chunk_size = 90
                for i in range(0, len(text), chunk_size):
                    yield f"data: {json.dumps({'type':'token','content': text[i:i+chunk_size]})}\n\n"
                memory.add(conv_id, "assistant", text[:1500])
            except Exception as _:
                err = f"[server error] {e}"
                yield f"data: {json.dumps({'type':'token','content': err})}\n\n"
            finally:
                yield "event: done\n"
                yield "data: {}\n\n"

    return StreamingResponse(sse_gen(), media_type="text/event-stream")

# í›„ì²˜ë¦¬ í•¨ìˆ˜ë“¤
def _strip_persona(text: str) -> str:
    """í˜ë¥´ì†Œë‚˜ ì œê±° (ìƒë‹´ì‚¬ì…ë‹ˆë‹¤... ì•ˆë…•í•˜ì„¸ìš”... ë„ì…ë¶€ ì»·)"""
    lines = text.splitlines()
    out, started = [], False
    persona = re.compile(r"(ìƒë‹´ì‚¬ì…ë‹ˆë‹¤|ì „ë¬¸ê°€ì…ë‹ˆë‹¤|ë„ì™€ë“œë¦¬ê² ìŠµë‹ˆë‹¤)", re.I)
    for ln in lines:
        if not started and (ln.strip().startswith("ì•ˆë…•í•˜ì„¸ìš”") or persona.search(ln)):
            continue
        started = True
        out.append(ln)
    return "\n".join(out).strip()

def _build_citations(ctx):
    """ì‹¤ì œ ì¸ìš© ê°•ì œ: ë¦¬íŠ¸ë¦¬ë²„ ìƒìœ„ ë¬¸ì„œì˜ (title, url)ë¡œ citations ì±„ì›€"""
    items, seen = [], set()
    for d in ctx[:3]:
        t, u = (d.get("title") or "").strip(), (d.get("url") or "").strip()
        if u and (t,u) not in seen:
            items.append(f"- ({t}) ({u})")
            seen.add((t,u))
    return "<citations>\n" + "\n".join(items) + "\n</citations>" if items else "<citations>\n</citations>"

def parse_answer_v3(text: str):
    """ìƒˆë¡œìš´ í˜•ì‹ íŒŒì„œ: ë¼ë²¨ ì—†ëŠ” ë¶ˆë¦¿ + ì°¸ê³  ë§í¬"""
    pattern = re.compile(r"""
    ^(?P<summary>[^\n]+)\n+                             # 1) ì²« ì¤„ ìš”ì•½
    (?P<tips>(?:-\s.*\n?)+)?                            # 2) ë¼ë²¨ ì—†ëŠ” ë¶ˆë¦¿ 0~N
    \n?\*\*(?:ì°¸ê³ |ê·¼ê±°)\*\*\s*\n                       # 3) ì°¸ê³ /ê·¼ê±° ë¼ë²¨(ê³ ì •)
    (?P<refs>(?:-\s.*\n?)+)?                            # 4) ì°¸ê³  ë§í¬ ë¶ˆë¦¿ 0~N
    \n?<followups>\s*\n                                 # 5) followups ì‹œì‘
    (?P<fups>(?:-\s.*\n?)+)                             # 6) í›„ì†ì§ˆë¬¸ 1~N
    \s*</followups>\s*$                                 # 7) ì¢…ë£Œ
    """, re.DOTALL | re.MULTILINE | re.UNICODE | re.VERBOSE)
    
    m = pattern.search(text.strip())
    if not m:
        return None
    
    tips = [l[2:].strip() for l in (m.group("tips") or "").splitlines() if l.strip().startswith("-")][:3]
    refs = [l[2:].strip() for l in (m.group("refs") or "").splitlines() if l.strip().startswith("-")][:2]
    fups = [l[2:].strip() for l in (m.group("fups") or "").splitlines() if l.strip().startswith("-")][:2]
    
    return {
        "summary": m.group("summary").strip(),
        "tips": tips,
        "refs": refs,
        "followups": fups,
    }

def _dedup_followups(text: str) -> str:
    """followups 2ê°œë¡œ ì •ë¦¬Â·ì¤‘ë³µ ì œê±°"""
    m = re.search(r"<followups>(.*?)</followups>", text, flags=re.S)
    if not m:
        return text
    uniq, out = set(), []
    for ln in m.group(1).splitlines():
        ln = ln.strip()
        if ln.startswith("-"):
            key = ln[1:].strip().lower()
            if key and key not in uniq:
                uniq.add(key)
                out.append(ln)
        if len(out) >= 2:
            break
    newblk = "<followups>\n" + ("\n".join(out) if out else "- ë‹¤ë¥¸ ì ë„ ë„ì™€ë“œë¦´ê¹Œìš”?") + "\n</followups>"
    return text[:m.start()] + newblk + text[m.end():]

@app.get("/debug/llm_status")
def debug_llm_status():
    """LLM ì„¤ì •ê³¼ ê°„ë‹¨í•œ í…ŒìŠ¤íŠ¸ë¥¼ ìˆ˜í–‰í•©ë‹ˆë‹¤."""
    try:
        # LLM ì„¤ì • í™•ì¸
        model_info = {
            "provider": settings.llm_provider,
            "model": settings.openai_chat_model if settings.llm_provider == "openai" else "gemini-1.5-flash",
            "api_key_set": bool(settings.openai_api_key if settings.llm_provider == "openai" else settings.gemini_api_key),
            "api_key_prefix": (settings.openai_api_key[:10] + "..." if settings.openai_api_key else "NOT SET") if settings.llm_provider == "openai" else (settings.gemini_api_key[:10] + "..." if settings.gemini_api_key else "NOT SET")
        }
        
        # ê°„ë‹¨í•œ LLM í…ŒìŠ¤íŠ¸
        test_messages = [
            {"role": "system", "content": "You are a helpful assistant."},
            {"role": "user", "content": "Say 'OK' only."}
        ]
        
        response = ""
        for delta in llm.stream_answer(test_messages):
            response += delta
            if len(response) > 10:  # ë„ˆë¬´ ê¸¸ë©´ ì¤‘ë‹¨
                break
        
        return {
            "status": "OK",
            "model_info": model_info,
            "test_response": response.strip(),
            "test_success": "OK" in response
        }
    except Exception as e:
        return {
            "status": "ERROR",
            "error": str(e),
            "model_info": {
                "provider": settings.llm_provider,
                "model": settings.openai_chat_model if settings.llm_provider == "openai" else "gemini-1.5-flash",
                "api_key_set": bool(settings.openai_api_key if settings.llm_provider == "openai" else settings.gemini_api_key),
                "api_key_prefix": (settings.openai_api_key[:10] + "..." if settings.openai_api_key else "NOT SET") if settings.llm_provider == "openai" else (settings.gemini_api_key[:10] + "..." if settings.gemini_api_key else "NOT SET")
            }
        }

@app.get("/debug/embedding_status")
def debug_embedding_status():
    """ì„ë² ë”© ëª¨ë¸ ìƒíƒœ í™•ì¸"""
    try:
        from .embeddings import LocalEmbedder
        embedder = LocalEmbedder()
        test_embedding = embedder.embed_one("test")
        
        return {
            "embed_model": embedder.model_name,
            "embed_dim": len(test_embedding),
            "index_dim": 1024,  # ChromaDB ì»¬ë ‰ì…˜ì—ì„œ í™•ì¸í•œ ì°¨ì›
            "dimension_match": len(test_embedding) == 1024,
            "config_model": settings.local_embed_model,
            "config_device": settings.local_embed_device
        }
    except Exception as e:
        return {"error": str(e)}

@app.get("/debug/env")
def debug_env():
    """í™˜ê²½ ë³€ìˆ˜ ë° ì‘ì—… ê²½ë¡œ í™•ì¸"""
    import os
    keys = ["USE_LOCAL_EMBEDDINGS","LOCAL_EMBED_MODEL","LOCAL_EMBED_DEVICE",
            "EXPECTED_EMBED_DIM","CHROMA_PATH","SQLITE_PATH","ENV_FILE"]
    return {
        "cwd": os.getcwd(),
        "env": {k: os.getenv(k) for k in keys}
    }

@app.get("/debug/parse_test")
def debug_parse_test():
    """ìƒˆë¡œìš´ íŒŒì„œ í…ŒìŠ¤íŠ¸"""
    test_text = """ë§Œ 14ì„¸ ì´ìƒì´ë©´ ìŠ¤ë§ˆíŠ¸ìŠ¤í† ì–´ë¥¼ ê°œì„¤í•  ìˆ˜ ìˆì–´ìš”.

- ë§Œ 14ì„¸~19ì„¸ëŠ” ë²•ì •ëŒ€ë¦¬ì¸ ë™ì˜ì™€ ì„œë¥˜ ì œì¶œì´ í•„ìš”í•´ìš”
- ë§Œ 19ì„¸ ì´ìƒì€ ì¼ë°˜ ê°œì„¤ì´ ê°€ëŠ¥í•´ìš”
- ê°œì¸ì‚¬ì—…ì ë“±ë¡ì´ í•„ìš”í•  ìˆ˜ ìˆì–´ìš”

**ì°¸ê³ **
- (ìŠ¤ë§ˆíŠ¸ìŠ¤í† ì–´ ê°€ì… ì ˆì°¨) (https://help.sell.smartstore.naver.com)
- (ê°œì¸ì‚¬ì—…ì ë“±ë¡ ì•ˆë‚´) (https://help.sell.smartstore.naver.com)

<followups>
- ê°œì¸ì‚¬ì—…ì ë“±ë¡ ë°©ë²•ì´ ê¶ê¸ˆí•´ìš”
- ì…ì  ì‹¬ì‚¬ ê¸°ê°„ì´ ì–¼ë§ˆë‚˜ ê±¸ë¦¬ë‚˜ìš”?
</followups>"""
    
    parsed = parse_answer_v3(test_text)
    return {
        "original": test_text,
        "parsed": parsed,
        "success": parsed is not None
    }

@app.get("/debug/model_info")
def debug_model_info():
    """ì‹¤ì œ ë¡œë“œë˜ëŠ” ëª¨ë¸ ì •ë³´ í™•ì¸"""
    try:
        from .embeddings import LocalEmbedder
        embedder = LocalEmbedder()
        test_embedding = embedder.embed_one("test")
        
        return {
            "model_name": embedder.model_name,
            "model_dim": embedder.dim,
            "test_embedding_shape": len(test_embedding),
            "config_model": settings.local_embed_model,
            "config_device": settings.local_embed_device,
            "model_loaded_successfully": True
        }
    except Exception as e:
        return {
            "error": str(e),
            "config_model": settings.local_embed_model,
            "config_device": settings.local_embed_device,
            "model_loaded_successfully": False
        }

@app.post("/debug/rebuild_dense")
def debug_rebuild_dense(batch_size: int = 256):
    try:
        out = retriever.rebuild_dense_from_docmap(batch_size=batch_size)
        return {"status": "ok", **out}
    except Exception as e:
        return {"status": "error", "error": str(e)}

@app.get("/debug/index_status")
def debug_index_status(q: str = "ìƒí’ˆ ë“±ë¡ ì ˆì°¨", k: int = 3):
    try:
        chroma_count = retriever.collection.count()
    except Exception:
        chroma_count = 0
    bm25_loaded = bool(retriever._bm25)
    doc_map_size = len(getattr(retriever, "_doc_map", {}) or {})
    dense_ok = bool(getattr(retriever, "dense_ok", False))

    preview = []
    try:
        hits = retriever.retrieve(q, k=k)
        for h in hits:
            preview.append({
                "id": h.get("id"),
                "title": h.get("title"),
                "score": round(float(h.get("score", 0.0)), 4),
                "has_url": bool(h.get("url")),
                "snippet": (h.get("text", "") or "")[:160]
            })
    except Exception:
        preview = []

    return {
        "chroma_path": settings.chroma_path,
        "chroma_count": chroma_count,
        "bm25_loaded": bm25_loaded,
        "doc_map_size": doc_map_size,
        "dense_ok": dense_ok,
        "sample_query": q,
        "preview": preview,
    }

@app.get("/conversations/{conversation_id}")
def get_history(conversation_id: str):
    rows = memory.fetch(conversation_id, limit=50)
    msgs = [Message(role=r, content=c) for r, c in rows]
    return ConversationHistory(conversation_id=conversation_id, messages=msgs)
